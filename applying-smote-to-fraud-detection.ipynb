{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a id='back_to_top'></a>\n\n# Applying SMOTE to Fraud Detection\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"**Created By**: Wuttipat S. <br>\n**Created Date**: 2023-09-24 <br>\n**Status**: <span style=\"color:green\">Completed</span>","metadata":{}},{"cell_type":"markdown","source":" <h3 style='background:green; color:#F0FFFF; text-align:center'><left>If you found my notebook helpful or informative, please consider upvoting it to show your support üëç</left></h3>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"text-align: center;\">\n    <img src=\"https://img.freepik.com/free-vector/hacker-activity-concept_23-2148534946.jpg?w=826&t=st=1695568933~exp=1695569533~hmac=2559706cefbc543821e7e6f48268a1e85672117841ae52b78937acee20149e72\" alt=\"Fraud Transaction\" width=\"500\"/>\n    <p style=\"text-align: center;\"><a href=\"https://www.freepik.com/free-vector/hacker-activity-concept_7970717.htm#query=fraud%20transaction&position=3&from_view=search&track=ais\">Image by freepik</a><p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"\n## Introduction\nIn this notebook, we explore the domain of **fraud detection**, a critical application in the banking and finance sectors. Especially, we experiment with **SMOTE (Synthetic Minority Over-sampling Technique)** to better understand its effects on measurement metrics in the fraud detection model.. \n\n<br>\n\n**This notebook outlines the process of fraud detection using a given dataset. The steps include:**\n1. data inspection\n1. exploratory data analysis\n1. preprocessing\n1. model training and evaluating\n     1. Base-line Model (LogisticRegression)\n     1. Applying SMOTE\n     1. Randomforst Classifier\n1. summary\n\n<br>\n\n**The dataset consists of the following columns:**\n- **step**: Integer value, possibly representing a time step or sequence.\n- **type**: Type of transaction (e.g., PAYMENT, TRANSFER, CASH_OUT).\n- **amount**: Amount involved in the transaction.\n- **nameOrig**: Originator of the transaction.\n- **oldbalanceOrg**: Initial balance before the transaction.\n- **newbalanceOrig**: New balance after the transaction.\n- **nameDest**: Recipient of the transaction.\n- **oldbalanceDest**: Initial balance of the recipient before the transaction.\n- **newbalanceDest**: New balance of the recipient after the transaction.\n-  **isFraud**: Binary label indicating if the transaction is fraudulent (1) or not (0).\n-  **isFlaggedFraud**: Binary label possibly indicating if the transaction was flagged as fraud by some system (1) or not (0).","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore') # Hide all warnings","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:00:58.875007Z","iopub.execute_input":"2023-09-24T17:00:58.875664Z","iopub.status.idle":"2023-09-24T17:00:58.887032Z","shell.execute_reply.started":"2023-09-24T17:00:58.875623Z","shell.execute_reply":"2023-09-24T17:00:58.886059Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nVertify what environment are running\n'''\nimport os\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n\nif iskaggle:\n    path='/kaggle/input/paysim1'\nelse:\n    path=\"{}\".format(os.getcwd())","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:00:58.888601Z","iopub.execute_input":"2023-09-24T17:00:58.889228Z","iopub.status.idle":"2023-09-24T17:00:58.904309Z","shell.execute_reply.started":"2023-09-24T17:00:58.889191Z","shell.execute_reply":"2023-09-24T17:00:58.902780Z"},"_kg_hide-input":false,"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import Python Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport time\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import precision_recall_fscore_support\n","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:00:58.906603Z","iopub.execute_input":"2023-09-24T17:00:58.908046Z","iopub.status.idle":"2023-09-24T17:01:00.283461Z","shell.execute_reply.started":"2023-09-24T17:00:58.907975Z","shell.execute_reply":"2023-09-24T17:01:00.281818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Data Inspection\nLet's look into the dataset.","metadata":{}},{"cell_type":"code","source":"# import the dataset\ndata = pd.read_csv(f\"{path}/PS_20174392719_1491204439457_log.csv\")\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:01:00.287342Z","iopub.execute_input":"2023-09-24T17:01:00.288173Z","iopub.status.idle":"2023-09-24T17:01:25.408539Z","shell.execute_reply.started":"2023-09-24T17:01:00.288110Z","shell.execute_reply":"2023-09-24T17:01:25.407258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  summary of the dataset.\ndata.info()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:01:25.409988Z","iopub.execute_input":"2023-09-24T17:01:25.410344Z","iopub.status.idle":"2023-09-24T17:01:25.443213Z","shell.execute_reply.started":"2023-09-24T17:01:25.410313Z","shell.execute_reply":"2023-09-24T17:01:25.441833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# descriptive statistics.\ndata.describe()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:01:25.444695Z","iopub.execute_input":"2023-09-24T17:01:25.445674Z","iopub.status.idle":"2023-09-24T17:01:27.702686Z","shell.execute_reply.started":"2023-09-24T17:01:25.445624Z","shell.execute_reply":"2023-09-24T17:01:27.700743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for missing values in the dataset\nmissing_values = data.isnull().sum()\nmissing_values","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:01:27.704465Z","iopub.execute_input":"2023-09-24T17:01:27.704883Z","iopub.status.idle":"2023-09-24T17:01:29.889622Z","shell.execute_reply.started":"2023-09-24T17:01:27.704850Z","shell.execute_reply":"2023-09-24T17:01:29.888169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Duplicated row\nduplicated = data.duplicated().sum()\nduplicated","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:01:29.890973Z","iopub.execute_input":"2023-09-24T17:01:29.891488Z","iopub.status.idle":"2023-09-24T17:01:51.496035Z","shell.execute_reply.started":"2023-09-24T17:01:29.891420Z","shell.execute_reply":"2023-09-24T17:01:51.494259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The dataset has a large number of entries (Over 6 millions rows).\n- Since our has not missing or duplicated values, the data cleaning is not needed.","metadata":{}},{"cell_type":"markdown","source":"---\n\n# 2. Exploratory Data Analysis\n\nNow, let's proceed with some Exploratory Data Analysis (EDA) to better understand the dataset. We'll:\n- Understand the distribution of the 'isFraud' column\n- Visualize the distribution of the 'isFraud' across transaction 'type'\n- Overview the 'FlaggedFraud' variable\n- Compare the correlation across all variables between 'isFruad' = 0 and 1\n- Vislualize the distribution of the 'amount' column","metadata":{}},{"cell_type":"code","source":"# Set the color palette\ncolors = sns.color_palette(\"pastel\")\n\ndata['isFraud'].value_counts().plot(kind='pie', autopct='%1.1f%%', startangle=0, colors=colors)\nplt.ylabel('')  # This removes the 'isFraud' label on the y-axis\nplt.title('Distribution of Fraudulent Transactions')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:17:46.154247Z","iopub.execute_input":"2023-09-24T17:17:46.154707Z","iopub.status.idle":"2023-09-24T17:17:46.396000Z","shell.execute_reply.started":"2023-09-24T17:17:46.154670Z","shell.execute_reply":"2023-09-24T17:17:46.393841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The target variable, 'isFraud', is imbalanced, with the vast majority of transactions being non-fraudulent.\n","metadata":{}},{"cell_type":"code","source":"# Create the countplot\nplt.figure(figsize=(6, 6))\nax = sns.countplot(x='type', data=data, hue='isFraud', palette='pastel')\n\n# Add a title and labels\nplt.title('Transaction Types vs. Fraud Count', fontsize=12)\nplt.xlabel('Transaction Type', fontsize=13)\nplt.ylabel('Count', fontsize=12)\n\n# Add count values above each bar\nfor p in ax.patches:\n    height = p.get_height()\n    if not np.isnan(height):  # Check if the height is not NaN and greater than 0\n        ax.annotate(f'{int(height)}', (p.get_x() + p.get_width() / 2., height),\n                    ha='center', va='center', fontsize=10, color='black',\n                    xytext=(0, 7),  # adjust the vertical offset\n                    textcoords='offset points')\n\n# Display the plot\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:18:25.690292Z","iopub.execute_input":"2023-09-24T17:18:25.690797Z","iopub.status.idle":"2023-09-24T17:18:34.946013Z","shell.execute_reply.started":"2023-09-24T17:18:25.690759Z","shell.execute_reply":"2023-09-24T17:18:34.944582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- `TRANSFER` and `CASH_OUT` are the only types of transactions that have a chance to be fraudulent.","metadata":{}},{"cell_type":"code","source":"flagged_data = data.loc[data['isFlaggedFraud']==1]\n\n# Create the countplot\nplt.figure(figsize=(6, 6))\nax = sns.countplot(x='isFlaggedFraud', data=data, hue='isFraud', palette='pastel')\n\n# Add a title and labels\nplt.title('Flagged Transaction Types vs. Fraud Count', fontsize=13)\nplt.ylabel('Count', fontsize=12)\n\n# Add count values above each bar\nfor p in ax.patches:\n    height = p.get_height()\n    if not np.isnan(height):  # Check if the height is not NaN and greater than 0\n        ax.annotate(f'{int(height)}', (p.get_x() + p.get_width() / 2., height),\n                    ha='center', va='center', fontsize=10, color='black',\n                    xytext=(0, 7),  # adjust the vertical offset\n                    textcoords='offset points')\n\n# Display the plot\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:18:41.462722Z","iopub.execute_input":"2023-09-24T17:18:41.463178Z","iopub.status.idle":"2023-09-24T17:18:43.007346Z","shell.execute_reply.started":"2023-09-24T17:18:41.463142Z","shell.execute_reply":"2023-09-24T17:18:43.005369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Out of all the transactions, only 16 were flagged as fraud. This small number indicates that the flagged system rarely detects all fraudulent transactions. However, if a transaction is flagged, it is definitively fraudulent.","metadata":{}},{"cell_type":"code","source":"temp = data.select_dtypes(include=['float64', 'int64'])\n\n# Compute the correlation matrix for 'data' dataframe\ncorr_data = temp.corr()\n\n# Compute the correlation matrix for 'fraudulent_transaction' dataframe\nfraudulent_transaction = temp.loc[data['isFraud']==1]\ncorr_fraudulent = fraudulent_transaction.corr()\n\n# Create subplots\nfig, ax = plt.subplots(1, 2, figsize=(15, 6))\n\n# Heatmap for 'data' dataframe\nsns.heatmap(corr_data, annot=False, fmt=\".2f\", cmap='coolwarm', ax=ax[0])\nax[0].set_title(\"All Transaction\", fontsize=12)\n\n# Heatmap for 'fraudulent_transaction' dataframe\nsns.heatmap(corr_fraudulent, annot=False, fmt=\".2f\", cmap='coolwarm', ax=ax[1])\nax[1].set_title(\"Fraudulent Transaction\", fontsize=12)\n\nfig.suptitle('Compare Heatmap Between All Transaction and Fraudulent Transaction', fontsize=15)\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:02:02.838859Z","iopub.execute_input":"2023-09-24T17:02:02.839674Z","iopub.status.idle":"2023-09-24T17:02:05.749268Z","shell.execute_reply.started":"2023-09-24T17:02:02.839618Z","shell.execute_reply":"2023-09-24T17:02:05.747880Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- There's a notable correlation between 'amount' and 'oldbalanceOrg' that distinguishes between all transactions and fraudulent transactions. This is typical for fraud because criminals are likely intending to empty the target's account regardless of the savings balance.","metadata":{}},{"cell_type":"code","source":"colors = sns.color_palette(\"pastel\")\n\nfig, axes = plt.subplots(nrows=2, ncols=1, figsize=(5, 8), sharex=True)\n\n# Distribution of Transaction Amounts for all transactions\naxes[0].hist(data['amount'], bins=50, color='skyblue', edgecolor='black')\naxes[0].set_title('All')\naxes[0].set_ylabel('Number of Transactions')\naxes[0].set_yscale('log')  # Set y-axis to log scale\naxes[0].grid(axis='y', linestyle='--', alpha=0.7)\n\n# Distribution of Transaction Amounts for fraudulent transactions\naxes[1].hist(data.loc[data['isFraud']==1]['amount'], bins=10, color='coral', edgecolor='black')\naxes[1].set_title('Fraudulent')\naxes[1].set_xlabel('Amount')\naxes[1].set_ylabel('Number of Transactions')\naxes[1].set_yscale('log')  # Set y-axis to log scale\naxes[1].grid(axis='y', linestyle='--', alpha=0.7)\n\nfig.suptitle('Distribution of Transaction Amounts', y=1)\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:02:05.754420Z","iopub.execute_input":"2023-09-24T17:02:05.754854Z","iopub.status.idle":"2023-09-24T17:02:08.128004Z","shell.execute_reply.started":"2023-09-24T17:02:05.754819Z","shell.execute_reply":"2023-09-24T17:02:08.126589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The distribution of the overall transaction amount is right-skewed, ranging between 0-70 million.\n- The distribution of fraudulent transaction amounts is also right-skewed, ranging between 0-10 million, with a spike at 10 million. This suggests that all fraudulent transactions do not exceed 10 million per transfer. This might be due to reaching the maximum transfer limit of the bank.","metadata":{}},{"cell_type":"markdown","source":"---\n# 3. Data Preprocessing\nBefore diving into model training, it's essential to preprocess the data to ensure it's in the right format for our machine learning algorithms. In this section:\n\n- We drop the unused columns.\n- Convert data types for *isFraud* and *isFlaggedFraud* to boolean type.\n- Encode the categorical columns using **OneHotEncoder**.\n- Further divide the data into training and testing sets. Stratified sampling is used to ensure that both sets have a similar distribution of the target variable.\n","metadata":{}},{"cell_type":"code","source":"# remove unuse columns\ndata.drop(['step', 'nameOrig', 'nameDest'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:02:08.129800Z","iopub.execute_input":"2023-09-24T17:02:08.130733Z","iopub.status.idle":"2023-09-24T17:02:08.361263Z","shell.execute_reply.started":"2023-09-24T17:02:08.130692Z","shell.execute_reply":"2023-09-24T17:02:08.359350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# covert data type of some columns\ndata['isFraud'] = data['isFraud'].astype(bool)\ndata['isFlaggedFraud'] = data['isFlaggedFraud'].astype(bool)\ndata.info()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:02:08.362922Z","iopub.execute_input":"2023-09-24T17:02:08.363287Z","iopub.status.idle":"2023-09-24T17:02:08.401114Z","shell.execute_reply.started":"2023-09-24T17:02:08.363258Z","shell.execute_reply":"2023-09-24T17:02:08.399375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n### OneHotEncoder\nIf we Label Encoding categorical variable (in this case 'type') as integer e.g. (0,1,2,..) algorithms might assume an ordinal relationhsip between the categories. For example\n- PAYMENT = 0\n- TRANSFER = 1\n- CASH_OUT = 2\n- DEBIT = 3\n- CASH_IN = 4\n    \nThis might lead the model to assume that DEBIT(3) somehow greater than TRANSFER(2), which doesn't make sense. OneHotEncoding avoids this problem.\n\n<div style=\"text-align: center;\">\n<img src=\"https://miro.medium.com/v2/resize:fit:720/0*T5jaa2othYfXZX9W.\" alt=\"Example of OneHotEncoder\" width=\"700\"/>\n<p style=\"text-align: center;\"><a href=\"https://medium.com/@michaeldelsole/what-is-one-hot-encoding-and-how-to-do-it-f0ae272f1179\">by Michael DelSole's Medium</a></p>\n</div>","metadata":{}},{"cell_type":"code","source":"# encodeing categorical vaiables\n\nencoder = OneHotEncoder(sparse_output=False)\n\n# Reshape the 'type' column to 2D array for encoder\ntype_col = data['type'].values.reshape(-1, 1)\n\nencoded_cols = encoder.fit_transform(type_col)\n\n# Reset the index of the data dataframe\ndata = data.reset_index(drop=True)\n\n# Create a DataFrame from the encoded columns\ndf_encoded = pd.DataFrame(encoded_cols, columns=encoder.get_feature_names_out(['type']))\n\n# Drop the original 'type' column and concatenate the encoded DataFrame\nencoded_data = data.drop(columns=['type'])\nencoded_data = pd.concat([encoded_data, df_encoded], axis=1)\n\n\nencoded_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:02:08.402696Z","iopub.execute_input":"2023-09-24T17:02:08.403052Z","iopub.status.idle":"2023-09-24T17:02:12.497585Z","shell.execute_reply.started":"2023-09-24T17:02:08.403022Z","shell.execute_reply":"2023-09-24T17:02:12.496319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n### Stratified Split\nAfter preprocessing the dataset, we'll divide it into a training set and a test set. Given the extreme imbalance in the dataset, the Stratified Split is a suitable choice for this approach.\n\n**Stratified Split** is a method used to partition a dataset into training and test sets, ensuring that the distribution of classes in the test set closely mirrors that of the training set.\n\n\n<div style=\"text-align: center;\">\n    <img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*EDS6v3bDOW1VQHdEVXdNQA.png\" alt=\"Example of OneHotEncoder\" width=\"500\"/>\n    <p style=\"text-align: center;\"><a href=\"https://medium.com/@analyttica/what-is-meant-by-stratified-split-289a8a986a90\">by Analyttica Datalab's Medium</a></p>\n</div>","metadata":{}},{"cell_type":"code","source":"# Split data into features and target\nX = encoded_data.drop(columns=['isFraud'])\ny = encoded_data['isFraud']\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:20:09.991717Z","iopub.execute_input":"2023-09-24T17:20:09.992252Z","iopub.status.idle":"2023-09-24T17:20:15.693967Z","shell.execute_reply.started":"2023-09-24T17:20:09.992212Z","shell.execute_reply":"2023-09-24T17:20:15.691958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n# 4. Training & Evaluating\n\nIn this section, we'll develop a function to train, test, and evaluate our model on various models and input data. Given the need to conduct multiple experiments to identify the most effective model for fraud detection, creating a custom function will streamline and simplify this process.","metadata":{}},{"cell_type":"code","source":"def run_model_and_evaluate(model, X_train, y_train, X_test, y_test):\n    \n    start_time = time.time()  # Start the timer\n\n    # Train the model\n    model.fit(X_train, y_train)\n\n    # Predict on test set\n    y_pred = model.predict(X_test)\n    y_pred_proba = model.predict_proba(X_test)[:, 1]  # Get the probability of the positive class\n    \n    # Stop the timer\n    end_time = time.time()\n    elapsed_time = end_time - start_time  # Calculate elapsed time in seconds\n\n    # Evaluate classifier's performance\n    accuracy = accuracy_score(y_test, y_pred)\n    roc_auc = roc_auc_score(y_test, y_pred_proba)  # Use probabilities to compute ROC AUC\n    \n    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n    \n    metrics_dict = {\n        'running_time': elapsed_time,\n        'accuracy': accuracy,\n        'roc_auc': roc_auc,\n        'precision': precision,\n        'recall': recall,\n        'f1_score': f1\n    }\n\n    print(f\"Running Time: {elapsed_time:.2f} seconds\")\n    print(f\"Accuracy: {accuracy}\")\n    print(f\"ROC AUC: {roc_auc}\")\n    print(classification_report(y_test, y_pred))\n\n    # Compute the confusion matrix\n    cm = confusion_matrix(y_test, y_pred)\n    \n    # ROC curve\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)    \n\n    fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n\n    # Confusion matrix\n    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', cbar=False,\n                xticklabels=[\"Not Fraud\", \"Fraud\"], yticklabels=[\"Not Fraud\", \"Fraud\"], ax=axes[0])\n    axes[0].set_xlabel('Predicted labels')\n    axes[0].set_ylabel('True labels')\n    axes[0].set_title('Confusion Matrix')\n\n    # ROC curve\n    axes[1].plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n    axes[1].plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n    axes[1].set_xlabel('False Positive Rate')\n    axes[1].set_ylabel('True Positive Rate')\n    axes[1].set_title('Receiver Operating Characteristic (ROC) Curve')\n    axes[1].legend(loc='lower right')\n    axes[1].grid(alpha=0.2)\n\n    plt.tight_layout()\n    plt.show()\n    \n    return metrics_dict\n","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:02:18.200783Z","iopub.execute_input":"2023-09-24T17:02:18.201409Z","iopub.status.idle":"2023-09-24T17:02:18.220192Z","shell.execute_reply.started":"2023-09-24T17:02:18.201373Z","shell.execute_reply":"2023-09-24T17:02:18.217843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n### 4.1 Baseline Model\nBefore diving deep into complex models, it's a good practice to start with a simple model to set a baseline. Here, we train a **Logistic Regression Classifier** on our data. The results highlight the importance of considering metrics beyond accuracy:","metadata":{}},{"cell_type":"code","source":"lr = run_model_and_evaluate(LogisticRegression(), X_train, y_train, X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:02:18.222757Z","iopub.execute_input":"2023-09-24T17:02:18.223946Z","iopub.status.idle":"2023-09-24T17:02:53.971329Z","shell.execute_reply.started":"2023-09-24T17:02:18.223896Z","shell.execute_reply":"2023-09-24T17:02:53.969637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here are the results from the logistic regression classifier trained on the balanced dataset:\n\n- **Running Time**: 21.18 sec\n- **Accuracy**: 99.79%\n- **ROC AUC**: 0.96\n- **Precision (for Fraudulent transactions)**: 0.36\n- **Recall (for Fraudulent transactions)**: 0.79\n- **F1-score (for Fraudulent transactions)**: 0.5\n\nLogistic Regression achieves high accuracy and ROC AUC, which means the model's overall reliability in classifying transactions is good. However, in fraud detection, where fraudulent transactions are intermingled with non-fraudulent ones, the balance between precision and recall often carries more weight than mere accuracy.\n\n- The recall is 0.79, indicating that the model can detect 79% of fraudulent activities. This is not ideal. In fraud detection, a high recall is critical to ensure that fewer fraudulent transactions go undetected.\n\n- The precision is 0.36, which means that for every 100 transactions the model predicts as fraudulent, only 36 of them are genuinely fraudulent. A lower precision implies that a higher number of legitimate transactions are incorrectly flagged.\n\n- The F1-score is reflecting the trade-off in fraud detection between capturing real fraudulent activities and minimizing false alarms. Since this is baseline model, a score of 0.5 indicates that there is room for improvement.","metadata":{}},{"cell_type":"markdown","source":"\n---\n\n### 4.2. Applying SMOTE\n\n\n**SMOTE - Synthetic Minority Over-sampling Technique**. It's a method used to handle class imbalance in datasets by specifically over-sampling the minority class.\n\n#### Why SMOTE?\nTo deal with the class imbalance problem, our choices usually revolve around Undersampling or Oversampling as a starting point. While Undersampling and Oversampling are popular choices:\n- Undersampling (decreasing the number of the majority class)\n- Oversampling (increasing the number of the minority class)\n\nThey both have drawbacks:\n- Undersampling can lead to a loss of valuable data.\n- Oversampling can lead to overfitting.\n\n#### How does SMOTE work?\nTo overcome these issues, SMOTE creates synthetic (not duplicate) samples in the dataset. This is essential because our data becomes balanced, akin to using Oversampling, but with a reduction in bias.\n\n<br><br>\n\n<div style=\"text-align: center;\">\n    <img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/1*CeOd_Wbn7O6kpjSTKTIUog.png\" alt=\"Example of OneHotEncoder\" width=\"700\"/>\n    <p style=\"text-align: center;\"><a href=\"https://medium.com/@parthdholakiya180/smote-synthetic-minority-over-sampling-technique-4d5a5d69d720\">by parth dholakiya's Medium</a></p>\n</div>","metadata":{}},{"cell_type":"code","source":"# Apply SMOTE to the training data\nsmote = SMOTE(random_state=42)\nX_train_smoted, y_train_smoted = smote.fit_resample(X_train, y_train)\n\nsmote_df = pd.concat([X_train_smoted, y_train_smoted], axis=1).reset_index(drop=True)\n\n# Checking the distribution of the target variable after SMOTE\nsmoted_distribution = y_train_smoted.value_counts()\n\nsmoted_distribution.plot(kind='bar', color=colors)\nplt.title('Count of SMOTE Training Set')\nplt.xticks(rotation=0)  # Rotate x-ticks by 90 degrees\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:21:01.624097Z","iopub.execute_input":"2023-09-24T17:21:01.624636Z","iopub.status.idle":"2023-09-24T17:21:08.370391Z","shell.execute_reply.started":"2023-09-24T17:21:01.624597Z","shell.execute_reply":"2023-09-24T17:21:08.369097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We have already generated synthetic fraudulent transactions. Plots show that our training set is now balanced.","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n\nfrac = 0.05 # Using a smaller fraction will reduce the execution time.\n\n# First subplot\nsns.scatterplot(x=\"newbalanceDest\", y=\"amount\", hue=\"isFraud\", data=data.sample(frac=frac, random_state=42), palette=\"pastel\", ax=axes[0])\naxes[0].set_title(\"Original Data\")\n\n# Second subplot\nsns.scatterplot(x=\"newbalanceDest\", y=\"amount\", hue=\"isFraud\", data=smote_df.sample(frac=frac, random_state=42), palette=\"pastel\", ax=axes[1])\naxes[1].set_title(\"SMOTE Data\")\n\n# Main title\nfig.suptitle('Scatter plot Comparison of Original and SMOTE Data (Sample with frac=0.05)', fontsize=15, y=1)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:03:00.414712Z","iopub.execute_input":"2023-09-24T17:03:00.415534Z","iopub.status.idle":"2023-09-24T17:03:33.216547Z","shell.execute_reply.started":"2023-09-24T17:03:00.415496Z","shell.execute_reply":"2023-09-24T17:03:33.215243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- From the plot, we observe that the fraudulent transactions are not merely duplicated. Instead, SMOTE creates new ones that are similar but not identical to the originals.\n\n---\n\nNext let's try out **LogisticRegressio**n model with **SMOTE's** training set","metadata":{}},{"cell_type":"code","source":"model = LogisticRegression(random_state=42, max_iter=100)\nlr_smote = run_model_and_evaluate(model, X_train_smoted, y_train_smoted, X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:03:33.218312Z","iopub.execute_input":"2023-09-24T17:03:33.218832Z","iopub.status.idle":"2023-09-24T17:04:18.135158Z","shell.execute_reply.started":"2023-09-24T17:03:33.218785Z","shell.execute_reply":"2023-09-24T17:04:18.133774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Using SMOTE has made the Logistic Regression model more sensitive to the minority class (in this case 'Fraud' = 1), leading to a significant increase in recall for fraudulent transactions. This model is now better at catching most of the fraudulent activities. However, this comes at the cost of reduced precision. The trade-off between Precision and Recall has now become clearer.\n\n<br><br>\n\nNext, we'll introduce an ensemble model (Random Forest) to improve our fraud detection.","metadata":{}},{"cell_type":"markdown","source":"---\n\n### 4.3 RandomForest\nWe will conduct experiments with two training sets:\n\n1. **RandomForest** with **imbalanced** training set.\n1. **RandomForest** with the **SMOTE** training set.","metadata":{}},{"cell_type":"code","source":"model = RandomForestClassifier(random_state=42, n_estimators=10)\nrf = run_model_and_evaluate(model, X_train, y_train, X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:04:18.136812Z","iopub.execute_input":"2023-09-24T17:04:18.141178Z","iopub.status.idle":"2023-09-24T17:07:01.676687Z","shell.execute_reply.started":"2023-09-24T17:04:18.141110Z","shell.execute_reply":"2023-09-24T17:07:01.675576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = RandomForestClassifier(random_state=42, n_estimators=10)\nrf_smote = run_model_and_evaluate(model, X_train_smoted, y_train_smoted, X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:07:01.678226Z","iopub.execute_input":"2023-09-24T17:07:01.678813Z","iopub.status.idle":"2023-09-24T17:13:52.435426Z","shell.execute_reply.started":"2023-09-24T17:07:01.678777Z","shell.execute_reply":"2023-09-24T17:13:52.434031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We have conducted several experiments with two models across two training sets.\n- Next, we'll compile the metrics of all the models we've trained into a single DataFrame for easier comparison.","metadata":{}},{"cell_type":"code","source":"result_df = pd.DataFrame([lr, lr_smote, rf, rf_smote], index=['LogisticRegression', 'LogisticRegression with SMOTE',\n                                                              'RandomForest', 'RandomForest with SMOTE'])\nresult_df","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:13:52.437414Z","iopub.execute_input":"2023-09-24T17:13:52.438244Z","iopub.status.idle":"2023-09-24T17:13:52.457277Z","shell.execute_reply.started":"2023-09-24T17:13:52.438200Z","shell.execute_reply":"2023-09-24T17:13:52.455313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n## Visualize metrics of all model","metadata":{}},{"cell_type":"code","source":"colors = sns.color_palette(\"pastel\")\n\nmax_value = result_df['running_time'].max()\ntemp = result_df.copy()\ntemp['normalize_running_time'] = temp['running_time'] / max_value\ntemp.drop('running_time', inplace=True, axis=1)\ntemp = temp.T","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:13:52.460134Z","iopub.execute_input":"2023-09-24T17:13:52.460848Z","iopub.status.idle":"2023-09-24T17:13:52.961511Z","shell.execute_reply.started":"2023-09-24T17:13:52.460799Z","shell.execute_reply":"2023-09-24T17:13:52.960309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = temp.plot(kind='bar', figsize=(10, 6), color=colors)\nax.legend(loc='upper left', bbox_to_anchor=(1, 1))  # Place the legend outside of the plot\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:13:52.460134Z","iopub.execute_input":"2023-09-24T17:13:52.460848Z","iopub.status.idle":"2023-09-24T17:13:52.961511Z","shell.execute_reply.started":"2023-09-24T17:13:52.460799Z","shell.execute_reply":"2023-09-24T17:13:52.960309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create color dictionary for the plot\nc = {\n    'lr':'#a1c9f4',\n    'lr_smote':'#ffb482',\n    'rf':'#8de5a1',\n    'rf_smote':'#ff9f9b',\n}\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# 1. Comparing LogisticRegression model between normal and smote\ntemp[['LogisticRegression', 'LogisticRegression with SMOTE']].plot(kind='bar', color=[c['lr'], c['lr_smote']], ax=axes[0, 0])\naxes[0, 0].set_title(\"LogisticRegression: Original vs. SMOTE\")\naxes[0, 0].set_ylabel(\"Metrics Value\")\naxes[0, 0].set_xticklabels(temp.index, rotation=45)\n\n# 2. Compare RandomForest model between normal and smote \ntemp[['RandomForest', 'RandomForest with SMOTE']].plot(kind='bar', color=[c['rf'], c['rf_smote']], ax=axes[0, 1])\naxes[0, 1].set_title(\"RandomForest: Original vs. SMOTE\")\naxes[0, 1].set_xticklabels(temp.index, rotation=45)\n\n# 3. Compare LogisticRegression model and RandomForest model normal\ntemp[['LogisticRegression', 'RandomForest']].plot(kind='bar', color=[c['lr'], c['rf']], ax=axes[1, 0])\naxes[1, 0].set_title(\"LogisticRegression vs. RandomForest (Original)\")\naxes[1, 0].set_ylabel(\"Metrics Value\")\naxes[1, 0].set_xticklabels(temp.index, rotation=45)\n\n# 4. compare LogisticRegression model and RandomForst model with smote\ntemp[['LogisticRegression with SMOTE', 'RandomForest with SMOTE']].plot(kind='bar', color=[c['lr_smote'], c['rf_smote']], ax=axes[1, 1])\naxes[1, 1].set_title(\"LogisticRegression vs. RandomForest (SMOTE)\")\naxes[1, 1].set_xticklabels(temp.index, rotation=45)\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-24T17:13:52.962984Z","iopub.execute_input":"2023-09-24T17:13:52.963558Z","iopub.status.idle":"2023-09-24T17:13:54.658205Z","shell.execute_reply.started":"2023-09-24T17:13:52.963526Z","shell.execute_reply":"2023-09-24T17:13:54.657116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Summary","metadata":{}},{"cell_type":"markdown","source":"- The RandomForest demonstrates robust performance, especially in the harmony of Precision and Recall, as reflected by the F1-score. However, it takes a longer runtime than Logistic Regression.\n- With SMOTE, while the precision for frauds decreases, the recall experiences a significant boost, indicating fewer missed frauds.\n- The trade-off between Precision and Recall is evident, and the choice of model would depend on the specific requirements of detection.","metadata":{}},{"cell_type":"markdown","source":" <h3 style='background:green; color:#F0FFFF; text-align:center'><left>If you found my notebook helpful or informative, please consider upvoting it to show your support üëç</left></h3>","metadata":{}},{"cell_type":"markdown","source":"## [Back to top](#back_to_top)","metadata":{}}]}